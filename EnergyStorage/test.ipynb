{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bb/opt/anaconda3/envs/dr/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd6cf12d6f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functorch import vmap\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from itertools import accumulate\n",
    "import cvxpy as cp\n",
    "import gurobipy\n",
    "import numpy as np\n",
    "from torch.autograd.functional import hessian\n",
    "import torch\n",
    "from torch.autograd import Function, Variable\n",
    "from torch.nn import Module\n",
    "from torch.nn.parameter import Parameter\n",
    "import util\n",
    "from collections import OrderedDict\n",
    "import plotly.graph_objs as go\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from baseline import MLP, RNNmodel\n",
    "import keras\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad torch.Size([50, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6.6757, -1.1579,  0.6692, -2.8934,  1.1456],\n",
       "         [-1.1579,  9.6575,  0.9183,  1.2770, -4.2600],\n",
       "         [ 0.6692,  0.9183,  5.1384, -0.1494, -1.5511],\n",
       "         [-2.8934,  1.2770, -0.1494,  9.6883, -1.3674],\n",
       "         [ 1.1456, -4.2600, -1.5511, -1.3674, 10.1102]]),\n",
       " tensor([-0.3840,  1.6371,  8.4603,  6.0742, -6.2592, 18.9336],\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ICNN(torch.nn.Module):\n",
    "    \"\"\"Input Convex Neural Network\"\"\"\n",
    "    def __init__(self, input_num=6, hidden_num=24):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_z1 = nn.Linear(hidden_num, 1, bias=False)\n",
    "        self.linear_y0 = nn.Linear(input_num, hidden_num)\n",
    "        self.linear_y1 = nn.Linear(input_num, 1)\n",
    "        torch.nn.init.uniform_(self.linear_z1.weight, a=1e-2,b=4)\n",
    "        torch.nn.init.uniform_(self.linear_y0.weight, a=-2,b=2)\n",
    "        #self.linear_y0.weight  = torch.nn.Parameter(torch.tril(torch.ones(24,24)))\n",
    "        #self.linear_z1.weight.data.fill_(1.0)\n",
    "        self.act = nn.Softplus()\n",
    "\n",
    "    def forward(self, y):\n",
    "        z1 = self.act(self.linear_y0(y))\n",
    "        z = self.act(self.linear_z1(z1) + self.linear_y1(y))\n",
    "        return z\n",
    "\n",
    "# Hessian: [T,B,1,1]\n",
    "# q: [T,B,1]\n",
    "torch.manual_seed(0)\n",
    "Cf = ICNN()\n",
    "B, T = 50, 6\n",
    "y = torch.rand(B,T)\n",
    "y2 = torch.rand(B,T)\n",
    "with torch.enable_grad():\n",
    "    tau = y.data\n",
    "    #tau2 = y2.data\n",
    "    #tau = torch.concat([y,y2],dim=1)\n",
    "    tau = Variable(tau, requires_grad=True)\n",
    "    cost = Cf(tau)\n",
    "    grad = torch.autograd.grad(cost.sum(), tau, create_graph=True, retain_graph=True)[0]\n",
    "    print('grad', grad.shape)\n",
    "    hessian = list()\n",
    "    for i in range(T):\n",
    "        hessian.append(\n",
    "            torch.autograd.grad(grad[:,i].sum(), tau,\n",
    "                                retain_graph=True)[0]\n",
    "        )\n",
    "    hessian = torch.stack(hessian, dim=-1)\n",
    "hessian[0][:5,:5], grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(B,T)\n",
    "y2 = torch.rand(B,T)\n",
    "torch.concat([y,y2],dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRagent(nn.Module):\n",
    "    \"\"\"\n",
    "    Using this layer, we train c1, c2, E1, E2, and eta, other parameters are infered from historical data.\n",
    "    \"\"\"\n",
    "    def __init__(self, P1, P2, T):\n",
    "        super().__init__()\n",
    "\n",
    "        self.E1 = nn.Parameter(0.5 * torch.ones(1))\n",
    "        self.E2 = nn.Parameter(-0.5 * torch.ones(1))\n",
    "        self.eta = nn.Parameter(0.9 * torch.ones(1))#0.89 * torch.ones(1) #nn.Parameter(0.9 * torch.ones(1))\n",
    "        self.T = T\n",
    "        eps = 1e-5\n",
    "\n",
    "        obj = (lambda d, p, price, c1, c2, E1, E2, eta: -price @ (d - p) + c1 @ d  +  cp.QuadForm(d, c2)\n",
    "            if isinstance(d, cp.Variable) #cp.sum_squares(cp.sqrt(cp.diag(c2)) @ d)\n",
    "            else -price @ (d - p) + c1 @ d  + d @ c2 @ d #torch.sum((torch.sqrt(torch.diag(c2)) @ d)**2)\n",
    "        )\n",
    "        self.objective = obj\n",
    "        # self.costNN = nn.Sequential(OrderedDict([('linear1',nn.Linear(1, 24)),\n",
    "        #             ('silu1',nn.SiLU()),\n",
    "        #             ('linear2',nn.Linear(24, 1))]))\n",
    "        # torch.nn.init.uniform_(self.costNN.linear1.weight, a=-2,b=-1e-2)\n",
    "        # torch.nn.init.uniform_(self.costNN.linear2.weight, a=1e-2, b=2)\n",
    "        # self.costNN.linear1.bias.data.fill_(0)\n",
    "        # self.costNN.linear2.bias.data.fill_(0)\n",
    "        self.costNN = ICNN()\n",
    "\n",
    "        self.ineq1 = (lambda d, p, price, c1, c2, E1, E2, eta: p - torch.ones(T, dtype=torch.double) * P1)\n",
    "        self.ineq2 = (lambda d, p, price, c1, c2, E1, E2, eta: torch.ones(T, dtype=torch.double)* 0 - p)\n",
    "        self.ineq3 = (lambda d, p, price, c1, c2, E1, E2, eta: d - torch.ones(T, dtype=torch.double) * P2)\n",
    "        self.ineq4 = (lambda d, p, price, c1, c2, E1, E2, eta: torch.ones(T, dtype=torch.double) * 0 - d)\n",
    "        self.ineq5 = (lambda d, p, price, c1, c2, E1, E2, eta: torch.tril(torch.ones(T, T, dtype=torch.double)) @ (eta * p - d / eta)\n",
    "            - torch.as_tensor(np.arange(eps, (T+1)*eps, eps))\n",
    "            - torch.ones(T, dtype=torch.double) * E1)\n",
    "        self.ineq6 = lambda d, p, price, c1, c2, E1, E2, eta: torch.ones(\n",
    "            T, dtype=torch.double\n",
    "        ) * E2 - torch.tril(torch.ones(T, T, dtype=torch.double)) @ (eta * p - d / eta) + torch.as_tensor(np.arange(eps, (T+1)*eps, eps))\n",
    "        \n",
    "        self.record = []\n",
    "        self.layer = util.OptLayer(\n",
    "            [cp.Variable(T), cp.Variable(T)],\n",
    "            [\n",
    "                cp.Parameter(T,),\n",
    "                cp.Parameter(T),\n",
    "                cp.Parameter((T,T), PSD=True),\n",
    "                cp.Parameter(1),\n",
    "                cp.Parameter(1),\n",
    "                cp.Parameter(1),\n",
    "            ],\n",
    "            obj,\n",
    "            [self.ineq1, self.ineq2, self.ineq3, self.ineq4, self.ineq5, self.ineq6],\n",
    "            [],\n",
    "            solver=\"GUROBI\",\n",
    "            verbose=False,\n",
    "        )\n",
    "    \n",
    "    def approximate_cost(self, y, diff=True):\n",
    "        # y: demand response [T,B,1]\n",
    "        Cf = self.costNN\n",
    "        with torch.enable_grad():\n",
    "            tau = y.data\n",
    "            #tau2 = p.data\n",
    "            #tau = torch.concat([tau,tau2],axis=1)\n",
    "            tau = Variable(tau, requires_grad=True)\n",
    "            cost = Cf(tau)\n",
    "            grad = torch.autograd.grad(cost.sum(), tau, create_graph=True, retain_graph=True)[0] # [B,T]\n",
    "            hessian = list()\n",
    "            for i in range(self.T):\n",
    "                hessian.append(torch.autograd.grad(grad[:,i].sum(), tau, retain_graph=True)[0])\n",
    "            hessian = torch.stack(hessian, dim=-1) # [B,T,T]\n",
    "            # if not diff:\n",
    "            #     return hessian.data, grad.data\n",
    "            # return hessian, grad\n",
    "            if not diff:\n",
    "                return hessian[:,:self.T,:].data, grad[:,:self.T].data\n",
    "            return hessian[:,:self.T,:], grad[:,:self.T]\n",
    "\n",
    "    \n",
    "    def dr_solver(self, *batch_params):\n",
    "        variables = [cp.Variable(T), cp.Variable(T)]\n",
    "        parameters = [cp.Parameter(T,), cp.Parameter(T), cp.Parameter(T), cp.Parameter(1), cp.Parameter(1), cp.Parameter(1),]\n",
    "        problem = cp.Problem(cp.Minimize(self.objective(*variables, *parameters)), [ineq(*variables, *parameters) <= 0 for ineq in [self.ineq1, self.ineq2, self.ineq3, self.ineq4, self.ineq5, self.ineq6]])\n",
    "        for batch in range(batch_params[0].shape[0]):\n",
    "            print(batch)\n",
    "            # solve the optimization problem\n",
    "            params = [p[batch] for p in batch_params]\n",
    "            with torch.no_grad():\n",
    "                for i, p in enumerate(parameters):\n",
    "                    p.value = params[i].double().numpy()\n",
    "                problem.solve(solver='GUROBI')\n",
    "                z = [torch.tensor(v.value).type_as(params[0]) for v in variables]\n",
    "                print(z[0].shape)\n",
    "\n",
    "\n",
    "    def forward(self, price, dr, ite):\n",
    "        # price: [B,T]\n",
    "        # dr: [B, T]\n",
    "        if ite < 500:\n",
    "            d = torch.rand(price.shape[0], price.shape[1]) # [B,T]\n",
    "            p = torch.rand(price.shape[0], price.shape[1])\n",
    "        else:\n",
    "            d = dr.data\n",
    "        #figure, axes = plt.subplots(1,1,figsize=(8,4))\n",
    "        for i in range(20):\n",
    "            #print(i)\n",
    "            c2, c1 = self.approximate_cost(d,  diff=False)\n",
    "            d, p = self.layer(price, \n",
    "                            c1,  c2,\n",
    "                            self.E1.expand(price.shape[0], *self.E1.shape),\n",
    "                            self.E2.expand(price.shape[0], *self.E2.shape),\n",
    "                            self.eta.expand(price.shape[0], *self.eta.shape), flag=True)\n",
    "            # plot_y = d.detach().numpy()[0] - p.detach().numpy()[0]\n",
    "            # axes.plot(plot_y,color='orange')\n",
    "            # anno = plt.annotate(f'step:{i}', xy=(0.8, 0.9), xycoords='axes fraction',color='black')\n",
    "            # plt.pause(0.01)\n",
    "            # axes.clear()\n",
    "            if ite < 10:\n",
    "                with torch.no_grad():\n",
    "                    u = torch.mean(torch.sum((p-d)*price, axis=1)).detach().numpy()\n",
    "            #d = d.permute(1,0).unsqueeze(2)\n",
    "            if ite < 10:\n",
    "                with torch.no_grad():\n",
    "                    f = self.costNN(d).detach().numpy()\n",
    "                    self.record.append(np.mean(f+u))\n",
    "                    \n",
    "        plt.pause(0)\n",
    "        \n",
    "        c2, c1 = self.approximate_cost(d,  diff=True) # c2: [B,T], c1: [B,T]\n",
    "\n",
    "        return self.layer(\n",
    "            price,\n",
    "            c1, \n",
    "            c2,\n",
    "            self.E1.expand(price.shape[0], *self.E1.shape),\n",
    "            self.E2.expand(price.shape[0], *self.E2.shape),\n",
    "            self.eta.expand(price.shape[0], *self.eta.shape),\n",
    "        ) # return: [2, B, T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (20x24 and 6x24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m layer \u001b[38;5;241m=\u001b[39m DRagent(P1, P2, T)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprice_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dr/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mDRagent.forward\u001b[0;34m(self, price, dr, ite)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m#figure, axes = plt.subplots(1,1,figsize=(8,4))\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m#print(i)\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     c2, c1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproximate_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdiff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     d, p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer(price, \n\u001b[1;32m    108\u001b[0m                     c1,  c2,\n\u001b[1;32m    109\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mE1\u001b[38;5;241m.\u001b[39mexpand(price\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mE1\u001b[38;5;241m.\u001b[39mshape),\n\u001b[1;32m    110\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mE2\u001b[38;5;241m.\u001b[39mexpand(price\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mE2\u001b[38;5;241m.\u001b[39mshape),\n\u001b[1;32m    111\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meta\u001b[38;5;241m.\u001b[39mexpand(price\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meta\u001b[38;5;241m.\u001b[39mshape), flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# plot_y = d.detach().numpy()[0] - p.detach().numpy()[0]\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# axes.plot(plot_y,color='orange')\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# anno = plt.annotate(f'step:{i}', xy=(0.8, 0.9), xycoords='axes fraction',color='black')\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# plt.pause(0.01)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# axes.clear()\u001b[39;00m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mDRagent.approximate_cost\u001b[0;34m(self, y, diff)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#tau2 = p.data\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#tau = torch.concat([tau,tau2],axis=1)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m tau \u001b[38;5;241m=\u001b[39m Variable(tau, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 65\u001b[0m cost \u001b[38;5;241m=\u001b[39m \u001b[43mCf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(cost\u001b[38;5;241m.\u001b[39msum(), tau, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# [B,T]\u001b[39;00m\n\u001b[1;32m     67\u001b[0m hessian \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dr/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mICNN.forward\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[0;32m---> 16\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_y0\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_z1(z1) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_y1(y))\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dr/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dr/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (20x24 and 6x24)"
     ]
    }
   ],
   "source": [
    "df_dp = np.load(\"dataset/data_N365_0.npz\")\n",
    "df_price = df_dp[\"price\"]\n",
    "\n",
    "T = 24\n",
    "N_train = 20\n",
    "P1 = df_dp[\"p\"].max()\n",
    "P2 = df_dp[\"d\"].max()\n",
    "d = df_dp[\"d\"]\n",
    "p = df_dp[\"p\"]\n",
    "price_tensor = torch.from_numpy(df_price[0:N_train]).double()\n",
    "d_tensor = torch.from_numpy(d[0:N_train]).double()\n",
    "p_tensor = torch.from_numpy(p[0:N_train]).double()\n",
    "y_tensor = tuple([d_tensor, p_tensor])\n",
    "\n",
    "torch.manual_seed(0)\n",
    "layer = DRagent(P1, P2, T)\n",
    "layer(price_tensor, d_tensor, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5 ,  0.5 ,  8.69, 10.62,  4.  ,  0.99]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = np.load(\"../EnergyStorage2/dataset/version2/data_N240_8.npz\")\n",
    "dataset['paras']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bb/opt/anaconda3/envs/dr/lib/python3.8/site-packages/cvxpy/reductions/solvers/solving_chain.py:178: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ite = 0, loss = 0.06032953013495276, eta = tensor([0.8800]), val_l = 0.07780543084571946\n",
      "ite = 1, loss = 0.05490032692475479, eta = tensor([0.8624]), val_l = 0.07275824631355166\n",
      "ite = 2, loss = 0.051488278692721254, eta = tensor([0.8468]), val_l = 0.06884065955032656\n",
      "ite = 3, loss = 0.04847619063874941, eta = tensor([0.8299]), val_l = 0.06511727594406312\n",
      "ite = 4, loss = 0.04461677085592913, eta = tensor([0.8143]), val_l = 0.0598304510435735\n",
      "ite = 5, loss = 0.041714824105118904, eta = tensor([0.7994]), val_l = 0.05594253378837519\n",
      "ite = 6, loss = 0.03969193158211852, eta = tensor([0.7863]), val_l = 0.053910268339662275\n",
      "ite = 7, loss = 0.03838771206382923, eta = tensor([0.7752]), val_l = 0.05236925694683975\n",
      "ite = 8, loss = 0.03729257434171254, eta = tensor([0.7657]), val_l = 0.05160327480063659\n",
      "ite = 9, loss = 0.03637285817897836, eta = tensor([0.7576]), val_l = 0.05107781887481447\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "dataset = np.load(\"../EnergyStorage2/dataset/version2/data_N240_8.npz\")\n",
    "df_price = dataset[\"price\"]\n",
    "\n",
    "T = 6\n",
    "N_train = 40\n",
    "P1 = dataset[\"p\"].max()\n",
    "P2 = dataset[\"d\"].max()\n",
    "d = dataset[\"d\"]\n",
    "p = dataset[\"p\"]\n",
    "price_tensor = torch.from_numpy(df_price[10:10+N_train]).double()\n",
    "d_tensor = torch.from_numpy(d[10:10+N_train]).double()\n",
    "p_tensor = torch.from_numpy(p[10:10+N_train]).double()\n",
    "y_tensor = tuple([d_tensor, p_tensor])\n",
    "\n",
    "price_tensor2 = torch.from_numpy(df_price[:10]).double()\n",
    "d_tensor2 = torch.from_numpy(d[:10]).double()\n",
    "p_tensor2 = torch.from_numpy(p[:10]).double()\n",
    "y_tensor2 = tuple([d_tensor2, p_tensor2])\n",
    "\n",
    "L = []\n",
    "val_L = []\n",
    "layer = DRagent(P1, P2, T)\n",
    "opt1 = optim.Adam(layer.parameters(), lr=2e-2)\n",
    "for ite in range(10):\n",
    "    dp_pred = layer(price_tensor, d_tensor, ite)\n",
    "    if ite == 10:\n",
    "        opt1.param_groups[0][\"lr\"] = 1e-2\n",
    "    elif ite == 100:\n",
    "        opt1.param_groups[0][\"lr\"] = 5e-3\n",
    "    loss = nn.MSELoss()(y_tensor[0], dp_pred[0]) + nn.MSELoss()(y_tensor[1], dp_pred[1])\n",
    "    opt1.zero_grad()\n",
    "    loss.backward()\n",
    "    opt1.step()\n",
    "    with torch.no_grad():\n",
    "        layer.E1.data = torch.clamp(layer.E1.data, min=0.01, max=100) \n",
    "        layer.E2.data = torch.clamp(layer.E2.data, min=-100, max=-0.01) \n",
    "        layer.eta.data =  torch.clamp(layer.eta.data, min=0.5, max=1) \n",
    "        if torch.min(layer.costNN.linear_z1.weight.data) < 1e-2:\n",
    "            layer.costNN.linear_z1.weight.data = torch.clamp(layer.costNN.linear_z1.weight.data, min=1e-2)\n",
    "    layer.eval()\n",
    "    dp_pred2 = layer(price_tensor2, d_tensor2, 500)\n",
    "    loss2 = nn.MSELoss()(y_tensor2[0], dp_pred2[0]) + nn.MSELoss()(y_tensor2[1], dp_pred2[1])\n",
    "    val_L.append(loss2.detach().numpy())\n",
    "    print(f'ite = {ite}, loss = {loss.detach().numpy()}, eta = {layer.eta.data}, val_l = {loss2.detach().numpy()}')\n",
    "    L.append(loss.detach().numpy())\n",
    "\n",
    "L = [float(l) for l in L]\n",
    "val_L = [float(l) for l in val_L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 400 into shape (10,20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 400 into shape (10,20)"
     ]
    }
   ],
   "source": [
    "np.array(layer.record).reshape(20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "import plotly\n",
    "from plotly.express.colors import sample_colorscale\n",
    "x = np.linspace(0, 1, 25)\n",
    "c = sample_colorscale('Greys', list(x))\n",
    "fig = go.Figure()\n",
    "for i in range(2,15):\n",
    "    fig.add_trace(go.Scatter(y=[layer.record[j]/abs(layer.record[i*10]) for j in range(i*10,(i+1)*10)], showlegend=False, line={'color':c[i+5]}))\n",
    "fig.update_layout({\n",
    "    'xaxis':{'title': r\"k\", 'showline':True},\n",
    "    'yaxis':{'title': r'$c(y^{(k)})/|c(y^{(0)})|$'},\n",
    "    #'yaxis2': {'title':'Price ($/MWh)','anchor':'x', 'overlaying':'y', 'side':'right', 'range':[0,40],'showgrid':False},\n",
    "    \"font\": {\"size\": 20,'color':'#3a4142'},\n",
    "    'template':'plotly_white',\n",
    "    'legend':{'yanchor':'bottom','y':1,'xanchor':'left','x':0.0,'orientation':'h'},\n",
    "    'width':800,\n",
    "    'height':500,\n",
    "    'coloraxis': {'colorscale':'viridis'}\n",
    "})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"result/images/convergence.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_d, pred_p = layer(price_tensor, d_tensor, 500)\n",
    "pred_d = pred_d.detach().numpy()\n",
    "pred_p = pred_p.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df_dp[\"d\"]\n",
    "p = df_dp[\"p\"]\n",
    "d2 = df_dp[\"d\"][:10]\n",
    "p2 = df_dp[\"p\"][:10]\n",
    "price_tensor2 = torch.from_numpy(df_price[:10]).double()\n",
    "d_tensor2 = torch.from_numpy(d[:10]).double()\n",
    "p_tensor2 = torch.from_numpy(p[:10]).double()\n",
    "pred_d2, pred_p2 = layer(price_tensor2, d_tensor2, 500)\n",
    "pred_d2 = pred_d2.detach().numpy()\n",
    "pred_p2 = pred_p2.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2, c1 = layer.approximate_cost(d_tensor, diff=True)\n",
    "torch.mean(torch.diag(c2[10])), torch.mean(c1[10]), layer.E1.data, layer.E2.data, layer.eta.data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag(c2[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(layer.state_dict(), '../EnergyStorage/result/model_gradient/ICNN_vector/model_gradient_model_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/bb/Desktop/UCSD/research/E2E/e2e-DR-learning/energystorage-model/Results/OptNet_val_loss.csv')\n",
    "opt_quad = df.values[:,1:]\n",
    "opt_quad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_mlp = np.zeros((10, 500))\n",
    "val_loss_rnn = np.zeros((10, 500))\n",
    "val_loss_model = np.zeros((10, 500))\n",
    "for i in range(10):\n",
    "    val_loss_mlp[i] = np.load(f'result/baseline/MLP_loss_{i}.npz')['val_loss']\n",
    "    val_loss_rnn[i] = np.load(f'result/baseline/RNN_loss_{i}.npz')['val_loss']\n",
    "for i in range(10):\n",
    "    val_loss_model[i] = np.load(f'result/model_gradient/loss_{i}.npz')['val_loss']\n",
    "\n",
    "fig = go.Figure()\n",
    "x1 = np.linspace(0,500,100)\n",
    "x2 = np.linspace(0,500,100)\n",
    "x3 = np.linspace(0,500,6)\n",
    "\n",
    "median_NN =  np.median(val_loss_mlp, axis=0)[:500]\n",
    "median_NN_2 = np.quantile(val_loss_mlp,0.2, axis=0)[:500]\n",
    "median_NN_8 = np.quantile(val_loss_mlp,0.8,axis=0)[:500]\n",
    "\n",
    "median_RNN =  np.median(val_loss_rnn, axis=0)[:500]\n",
    "median_RNN_2 = np.quantile(val_loss_rnn,0.2,axis=0)[:500]\n",
    "median_RNN_8 = np.quantile(val_loss_rnn,0.8,axis=0)[:500]\n",
    "\n",
    "median_opt0 =  np.load(f'result/model_gradient/ICNN_vector/loss_0.npz')['val_loss']#np.median(opt_quad, axis=0)\n",
    "median_opt0_2 = np.quantile(opt_quad, 0.2, axis=0)\n",
    "median_opt0_8 = np.quantile(opt_quad, 0.8, axis=0)\n",
    "\n",
    "median_opt1 =  np.median(val_loss_model, axis=0)[:500]\n",
    "median_opt1_2 = np.quantile(val_loss_model, 0.2, axis=0)\n",
    "median_opt1_8 = np.quantile(val_loss_model, 0.8, axis=0)\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x1, y=median_NN_2,fill=None,mode='lines',line_color='rgba(0,0,0,0)', showlegend = False ))\n",
    "fig.add_trace(go.Scatter(x=x1,y=median_NN_8,fill='tonexty', mode='lines',  fillcolor='rgba(203,158,252,0.2)',line_color = 'rgba(0,0,0,0)',showlegend = False))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x1, y=median_RNN_2,fill=None,mode='lines',line_color='rgba(0,0,0,0)', showlegend = False ))\n",
    "fig.add_trace(go.Scatter(x=x1,y=median_RNN_8,fill='tonexty', mode='lines',  fillcolor='rgba(168,236,217,0.5)',line_color='rgba(0,0,0,0)',showlegend = False))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x2, y=median_opt1_2,fill=None,mode='lines',line_color='rgba(0,0,0,0)', showlegend = False    ))\n",
    "fig.add_trace(go.Scatter(x=x2,y=median_opt1_8,fill='tonexty', mode='lines',  fillcolor='rgba(240,96,72,0.5)',line_color = 'rgba(0,0,0,0)',showlegend = False))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x3, y=median_opt0_2,fill=None,mode='lines',line_color='rgba(0,0,0,0)', showlegend = False    ))\n",
    "fig.add_trace(go.Scatter(x=x3,y=median_opt0_8,fill='tonexty', mode='lines',  fillcolor='rgba(250,232,20,0.5)',line_color = 'rgba(0,0,0,0)',showlegend = False))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x1,y=median_NN, mode='lines',  line_color='#b681f0',opacity=0.5, line_width=3,name = 'median(MLP,300)')) \n",
    "fig.add_trace(go.Scatter(x=x1,y=median_RNN, mode='lines',  line_color='#07cd98',opacity=0.7,line_width=3,name = 'median(RNN,300)')) \n",
    "fig.add_trace(go.Scatter(x=x2,y=median_opt1, mode='lines',  line_color='#f06048',opacity=0.5,line_width=3,name = 'median(Ours-nonconvex,20)')) \n",
    "fig.add_trace(go.Scatter(x=x1,y=median_opt0, mode='lines',  line_color='#fff50e',opacity=0.5,line_width=3,name = 'median(Ours-convex,20)')) \n",
    "\n",
    "fig.update_layout({#'title': 'NRMSE comparison',\n",
    "    'xaxis':{'title':'Iterations', 'showline':True},\n",
    "    'yaxis':{'title': 'Validation Loss',\"range\":[0,0.05]},\n",
    "    \"font\": {\"size\": 40, 'color':'#3a4142'},\n",
    "    'template':'plotly_white',\n",
    "    'legend':{'yanchor':'bottom','y':1,'xanchor':'left','x':0.0,'orientation':'h'},\n",
    "    'width':1600,\n",
    "   'height':900\n",
    "})\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"result/images/validation_loss.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"result/images/test_1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnmodel = keras.models.load_model('result/baseline/model_RNN_1')\n",
    "mlpmodel = keras.models.load_model('result/baseline/model_MLP_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from differentiable_DR_model import DRagent\n",
    "dataset = np.load(f\"dataset/data_N365_1.npz\")\n",
    "P1 = dataset[\"p\"].max()\n",
    "P2 = dataset[\"d\"].max()\n",
    "T = 24\n",
    "model = DRagent(P1, P2, T, type='vector')\n",
    "model.load_state_dict(torch.load('result/model_gradient/ICNN_vector/model_1.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_price = dataset['price'][:20]\n",
    "df_dp = dataset\n",
    "d2 = df_dp[\"d\"][:20]\n",
    "p2 = df_dp[\"p\"][:20]\n",
    "test_dr = p2-d2\n",
    "price_tensor2 = torch.from_numpy(test_price).double()\n",
    "d_tensor2 = torch.from_numpy(d2).double()\n",
    "p_tensor2 = torch.from_numpy(p2).double()\n",
    "pred_d2, pred_p2 = model(price_tensor2, d_tensor2, 0)\n",
    "pred_d2 = pred_d2.detach().numpy()\n",
    "pred_p2 = pred_p2.detach().numpy()\n",
    "pred_dr = pred_p2-pred_d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(10)\n",
    "d = df_dp[\"d\"]\n",
    "p = df_dp[\"p\"]\n",
    "df_price = df_dp['price']\n",
    "test_price = df_price[:10]\n",
    "\n",
    "dr1 = rnnmodel(df_price[i:i+1])\n",
    "dr3 = mlpmodel(df_price[i:i+1])\n",
    "test_dr = p[:10] - d[:10]\n",
    "pred_dr = pred_p2 - pred_d2 \n",
    "t1 = go.Scatter(x=np.arange(24), y=test_price[i],mode='lines', name='price',line = {'width':6,'dash':'dash','color':'#ffa05a'},yaxis='y2')\n",
    "t2 = go.Scatter(x=np.arange(24), y=test_dr[i],mode='lines', name='True',line = {'width':6,'dash':'dash'})\n",
    "t5 = go.Scatter(x=np.arange(24), y=pred_dr[i],mode='lines', name='Ours(non-convex)',line = {'width':4})\n",
    "t3 = go.Scatter(x=np.arange(24), y=dr1[0],mode='lines', name='RNN(200)',line = {'width':4})\n",
    "t6 = go.Scatter(x=np.arange(24), y=dr3[0],mode='lines', name='MLP(200)',line = {'width':4})\n",
    "fig = go.Figure(data=[t2,t5, t3,t6,t1],layout={\n",
    "    'xaxis':{'title':'Hour', 'showline':True},\n",
    "    'yaxis':{'title': 'Dispatch(MW)','range':[-0.6,0.5]},\n",
    "    'yaxis2': {'title':'Price ($/MWh)','anchor':'x', 'overlaying':'y', 'side':'right','range':[10,100], 'showgrid':False},\n",
    "    \"font\": {\"size\": 40,'color':'#3a4142'},\n",
    "    'template':'plotly_white',\n",
    "    'legend':{'yanchor':'bottom','y':1,'xanchor':'left','x':0.0,'orientation':'h'},\n",
    "    'width':1600,\n",
    "    'height':900\n",
    "})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"result/images/predict.pdf\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5cc9548c05a244386a6d3194fd4ce0daff846e301aca00a00f7a9d28040987a3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('dr': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
