{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bb/opt/anaconda3/envs/dr/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe2d35a74f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functorch import vmap\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from itertools import accumulate\n",
    "import cvxpy as cp\n",
    "import gurobipy\n",
    "import numpy as np\n",
    "from torch.autograd.functional import hessian\n",
    "import torch\n",
    "from torch.autograd import Function, Variable\n",
    "from torch.nn import Module\n",
    "from torch.nn.parameter import Parameter\n",
    "import util\n",
    "from collections import OrderedDict\n",
    "import plotly.graph_objs as go\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from baseline import MLP, RNNmodel\n",
    "import keras\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad torch.Size([50, 24])\n",
      "hessian torch.Size([50, 24, 24])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.9989, 3.8223, 5.7362, 5.7431, 6.1441, 2.6767, 4.9081, 6.6186, 3.5747,\n",
       "         2.7404, 5.0469, 6.3632, 2.5757, 3.9986, 2.1191, 2.5184, 2.4527, 4.6397,\n",
       "         5.3188, 3.6995, 2.0550, 1.7860, 2.4729, 2.9265]),\n",
       " 50)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ICNN(torch.nn.Module):\n",
    "    \"\"\"Input Convex Neural Network\"\"\"\n",
    "    def __init__(self, input_num=24, hidden_num=24):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_z1 = nn.Linear(hidden_num, 1, bias=False)\n",
    "        self.linear_y0 = nn.Linear(input_num, hidden_num)\n",
    "        self.linear_y1 = nn.Linear(input_num, 1)\n",
    "        torch.nn.init.uniform_(self.linear_z1.weight, a=1e-2,b=4)\n",
    "        torch.nn.init.uniform_(self.linear_y0.weight, a=-2,b=2)\n",
    "        self.act = nn.Softplus()\n",
    "\n",
    "    def forward(self, y):\n",
    "        z1 = self.act(self.linear_y0(y))\n",
    "        z = self.act(self.linear_z1(z1) + self.linear_y1(y))\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "# Hessian: [T,B,1,1]\n",
    "# q: [T,B,1]\n",
    "Cf = ICNN()\n",
    "B, T = 50, 24\n",
    "y = torch.rand(B,T)\n",
    "with torch.enable_grad():\n",
    "    tau = y.data\n",
    "    tau = Variable(tau, requires_grad=True)\n",
    "    cost = Cf(tau)\n",
    "    grad = torch.autograd.grad(cost.sum(), tau, create_graph=True, retain_graph=True)[0]\n",
    "    print('grad', grad.shape)\n",
    "    hessian = list()\n",
    "    for i in range(T):\n",
    "        hessian.append(\n",
    "            torch.autograd.grad(grad[:,i].sum(), tau,\n",
    "                                retain_graph=True)[0]\n",
    "        )\n",
    "    \n",
    "    hessian = torch.stack(hessian, dim=-1)\n",
    "    print('hessian', hessian.shape)\n",
    "torch.diag(hessian[0]), grad.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRagent(nn.Module):\n",
    "    \"\"\"\n",
    "    Using this layer, we train c1, c2, E1, E2, and eta, other parameters are infered from historical data.\n",
    "    \"\"\"\n",
    "    def __init__(self, P1, P2, T):\n",
    "        super().__init__()\n",
    "\n",
    "        self.E1 = nn.Parameter(0.5 * torch.ones(1))\n",
    "        self.E2 = nn.Parameter(-0.5 * torch.ones(1))\n",
    "        self.eta = nn.Parameter(0.9 * torch.ones(1))\n",
    "        self.T = T\n",
    "        eps = 1e-5\n",
    "\n",
    "        obj = (\n",
    "            lambda d, p, price, c1, c2, E1, E2, eta: -price @ (d - p)\n",
    "            + c1 @ d  +  cp.QuadForm(d, c2)\n",
    "            if isinstance(d, cp.Variable) #cp.sum_squares(cp.sqrt(cp.diag(c2)) @ d)\n",
    "            else -price @ (d - p) + c1 @ d +  d @ c2 @ d #torch.sum((torch.sqrt(torch.diag(c2)) @ d)**2)\n",
    "        )\n",
    "        self.objective = obj\n",
    "        # self.costNN = nn.Sequential(OrderedDict([('linear1',nn.Linear(1, 24)),\n",
    "        #             ('silu1',nn.SiLU()),\n",
    "        #             ('linear2',nn.Linear(24, 1))]))\n",
    "        # torch.nn.init.uniform_(self.costNN.linear1.weight, a=-2,b=-1e-2)\n",
    "        # torch.nn.init.uniform_(self.costNN.linear2.weight, a=1e-2, b=2)\n",
    "        # self.costNN.linear1.bias.data.fill_(0)\n",
    "        # self.costNN.linear2.bias.data.fill_(0)\n",
    "        self.costNN = ICNN()\n",
    "\n",
    "        self.ineq1 = (lambda d, p, price, c1, c2, E1, E2, eta: p - torch.ones(T, dtype=torch.double) * P1)\n",
    "        self.ineq2 = (lambda d, p, price, c1, c2, E1, E2, eta: torch.ones(T, dtype=torch.double)* 0 - p)\n",
    "        self.ineq3 = (lambda d, p, price, c1, c2, E1, E2, eta: d - torch.ones(T, dtype=torch.double) * P2)\n",
    "        self.ineq4 = (lambda d, p, price, c1, c2, E1, E2, eta: torch.ones(T, dtype=torch.double) * 0 - d)\n",
    "        self.ineq5 = (lambda d, p, price, c1, c2, E1, E2, eta: torch.tril(torch.ones(T, T, dtype=torch.double)) @ (eta * p - d / eta)\n",
    "            - torch.as_tensor(np.arange(eps, (T+1)*eps, eps))\n",
    "            - torch.ones(T, dtype=torch.double) * E1)\n",
    "        self.ineq6 = lambda d, p, price, c1, c2, E1, E2, eta: torch.ones(\n",
    "            T, dtype=torch.double\n",
    "        ) * E2 - torch.tril(torch.ones(T, T, dtype=torch.double)) @ (eta * p - d / eta) + torch.as_tensor(np.arange(eps, (T+1)*eps, eps))\n",
    "        \n",
    "        self.record = []\n",
    "        self.layer = util.OptLayer(\n",
    "            [cp.Variable(T), cp.Variable(T)],\n",
    "            [\n",
    "                cp.Parameter(T,),\n",
    "                cp.Parameter(T),\n",
    "                cp.Parameter((T,T), PSD=True),\n",
    "                cp.Parameter(1),\n",
    "                cp.Parameter(1),\n",
    "                cp.Parameter(1),\n",
    "            ],\n",
    "            obj,\n",
    "            [self.ineq1, self.ineq2, self.ineq3, self.ineq4, self.ineq5, self.ineq6],\n",
    "            [],\n",
    "            solver=\"GUROBI\",\n",
    "            verbose=False,\n",
    "        )\n",
    "    \n",
    "    def approximate_cost(self, y, diff=True):\n",
    "        # y: demand response [T,B,1]\n",
    "        Cf = self.costNN\n",
    "        with torch.enable_grad():\n",
    "            tau = y.data\n",
    "            tau = Variable(tau, requires_grad=True)\n",
    "            cost = Cf(tau)\n",
    "            grad = torch.autograd.grad(cost.sum(), tau, create_graph=True, retain_graph=True)[0] # [B,T]\n",
    "            hessian = list()\n",
    "            for i in range(self.T):\n",
    "                hessian.append(torch.autograd.grad(grad[:,i].sum(), tau, retain_graph=True)[0])\n",
    "            hessian = torch.stack(hessian, dim=-1) # [B,T,T]\n",
    "            if not diff:\n",
    "                return hessian.data, grad.data\n",
    "            return hessian, grad\n",
    "\n",
    "    \n",
    "    def dr_solver(self, *batch_params):\n",
    "        variables = [cp.Variable(T), cp.Variable(T)]\n",
    "        parameters = [cp.Parameter(T,), cp.Parameter(T), cp.Parameter(T), cp.Parameter(1), cp.Parameter(1), cp.Parameter(1),]\n",
    "        problem = cp.Problem(cp.Minimize(self.objective(*variables, *parameters)), [ineq(*variables, *parameters) <= 0 for ineq in [self.ineq1, self.ineq2, self.ineq3, self.ineq4, self.ineq5, self.ineq6]])\n",
    "        for batch in range(batch_params[0].shape[0]):\n",
    "            print(batch)\n",
    "            # solve the optimization problem\n",
    "            params = [p[batch] for p in batch_params]\n",
    "            with torch.no_grad():\n",
    "                for i, p in enumerate(parameters):\n",
    "                    p.value = params[i].double().numpy()\n",
    "                problem.solve(solver='GUROBI')\n",
    "                z = [torch.tensor(v.value).type_as(params[0]) for v in variables]\n",
    "                print(z[0].shape)\n",
    "\n",
    "\n",
    "    def forward(self, price, dr, ite):\n",
    "        # price: [B,T]\n",
    "        # dr: [B, T]\n",
    "        if ite < 500:\n",
    "            d = torch.rand(price.shape[0], price.shape[1]) # [B,T]\n",
    "        else:\n",
    "            d = dr.data\n",
    "        #figure, axes = plt.subplots(1,1,figsize=(8,4))\n",
    "        for i in range(10):\n",
    "            #print(i)\n",
    "            c2, c1 = self.approximate_cost(d, diff=False)\n",
    "            d, p = self.layer(price, c1, c2,\n",
    "                            self.E1.expand(price.shape[0], *self.E1.shape),\n",
    "                            self.E2.expand(price.shape[0], *self.E2.shape),\n",
    "                            self.eta.expand(price.shape[0], *self.eta.shape), flag=True)\n",
    "            # plot_y = d.detach().numpy()[0] - p.detach().numpy()[0]\n",
    "            # axes.plot(plot_y,color='orange')\n",
    "            # anno = plt.annotate(f'step:{i}', xy=(0.8, 0.9), xycoords='axes fraction',color='black')\n",
    "            # plt.pause(0.01)\n",
    "            # axes.clear()\n",
    "            if ite > 500:\n",
    "                with torch.no_grad():\n",
    "                    u = torch.mean(torch.sum((p-d)*price, axis=1)).detach().numpy()\n",
    "            #d = d.permute(1,0).unsqueeze(2)\n",
    "            if ite > 500:\n",
    "                with torch.no_grad():\n",
    "                    f = torch.mean(torch.sum(torch.stack([self.costNN(d[t]) for t in range(24)]),axis=0)).detach().numpy()\n",
    "                    self.record.append(f+u)\n",
    "                    print(len(self.record))\n",
    "        plt.pause(0)\n",
    "        \n",
    "        c2, c1 = self.approximate_cost(d, diff=True) # c2: [B,T], c1: [B,T]\n",
    "\n",
    "        return self.layer(\n",
    "            price,\n",
    "            c1, \n",
    "            c2,\n",
    "            self.E1.expand(price.shape[0], *self.E1.shape),\n",
    "            self.E2.expand(price.shape[0], *self.E2.shape),\n",
    "            self.eta.expand(price.shape[0], *self.eta.shape),\n",
    "        ) # return: [2, B, T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dp = np.load(\"dataset/data_N365_0.npz\")\n",
    "df_price = df_dp[\"price\"]\n",
    "\n",
    "T = 24\n",
    "N_train = 20\n",
    "P1 = df_dp[\"p\"].max()\n",
    "P2 = df_dp[\"d\"].max()\n",
    "d = df_dp[\"d\"]\n",
    "p = df_dp[\"p\"]\n",
    "price_tensor = torch.from_numpy(df_price[0:N_train]).double()\n",
    "d_tensor = torch.from_numpy(d[0:N_train]).double()\n",
    "p_tensor = torch.from_numpy(p[0:N_train]).double()\n",
    "y_tensor = tuple([d_tensor, p_tensor])\n",
    "\n",
    "torch.manual_seed(0)\n",
    "layer = DRagent(P1, P2, T)\n",
    "layer(price_tensor, d_tensor, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.linalg.eigvals(np.zeros((24,24)))>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n",
      "ite = 0, loss = 0.027564408075990973, eta = tensor([0.8900])\n",
      "ite = 1, loss = 0.023158136350357298, eta = tensor([0.8802])\n",
      "ite = 2, loss = 0.01972937102944846, eta = tensor([0.8710])\n",
      "ite = 3, loss = 0.0176954192950445, eta = tensor([0.8618])\n",
      "ite = 4, loss = 0.015655627244304963, eta = tensor([0.8523])\n",
      "ite = 5, loss = 0.01299658056997367, eta = tensor([0.8435])\n",
      "ite = 6, loss = 0.012007880832921704, eta = tensor([0.8352])\n",
      "ite = 7, loss = 0.010699490825133598, eta = tensor([0.8275])\n",
      "ite = 8, loss = 0.010072984754780119, eta = tensor([0.8203])\n",
      "ite = 9, loss = 0.009173767126323252, eta = tensor([0.8135])\n",
      "ite = 10, loss = 0.008681048679219927, eta = tensor([0.8104])\n",
      "ite = 11, loss = 0.008452658871062065, eta = tensor([0.8075])\n",
      "ite = 12, loss = 0.008241894199053613, eta = tensor([0.8048])\n",
      "ite = 13, loss = 0.00798561966118638, eta = tensor([0.8022])\n",
      "ite = 14, loss = 0.007641864069817532, eta = tensor([0.7997])\n",
      "ite = 15, loss = 0.0073296674697131475, eta = tensor([0.7973])\n",
      "ite = 16, loss = 0.00712146950358883, eta = tensor([0.7950])\n",
      "ite = 17, loss = 0.006998616163673922, eta = tensor([0.7931])\n",
      "ite = 18, loss = 0.006936514930086093, eta = tensor([0.7914])\n",
      "ite = 19, loss = 0.006874403489223614, eta = tensor([0.7902])\n",
      "ite = 20, loss = 0.006841102139061996, eta = tensor([0.7892])\n",
      "ite = 21, loss = 0.0068119491699284735, eta = tensor([0.7886])\n",
      "ite = 22, loss = 0.00677493406836579, eta = tensor([0.7884])\n",
      "ite = 23, loss = 0.006720374546666025, eta = tensor([0.7884])\n",
      "ite = 24, loss = 0.006641143403800906, eta = tensor([0.7888])\n",
      "ite = 25, loss = 0.0065459108846339115, eta = tensor([0.7893])\n",
      "ite = 26, loss = 0.006438546592993944, eta = tensor([0.7901])\n",
      "ite = 27, loss = 0.006323503465644236, eta = tensor([0.7911])\n",
      "ite = 28, loss = 0.00620457688006383, eta = tensor([0.7922])\n",
      "ite = 29, loss = 0.006085818986377717, eta = tensor([0.7935])\n",
      "ite = 30, loss = 0.005967715770367075, eta = tensor([0.7948])\n",
      "ite = 31, loss = 0.005854445207860156, eta = tensor([0.7962])\n",
      "ite = 32, loss = 0.005751210935450441, eta = tensor([0.7977])\n",
      "ite = 33, loss = 0.005660118884482397, eta = tensor([0.7991])\n",
      "ite = 34, loss = 0.005573910791528042, eta = tensor([0.8006])\n",
      "ite = 35, loss = 0.005512318407745014, eta = tensor([0.8019])\n",
      "ite = 36, loss = 0.00547475508888326, eta = tensor([0.8031])\n",
      "ite = 37, loss = 0.005455602134767765, eta = tensor([0.8040])\n",
      "ite = 38, loss = 0.005443598856062171, eta = tensor([0.8048])\n",
      "ite = 39, loss = 0.0054234343906925265, eta = tensor([0.8054])\n",
      "ite = 40, loss = 0.005383118193023465, eta = tensor([0.8058])\n",
      "ite = 41, loss = 0.005343577159805337, eta = tensor([0.8062])\n",
      "ite = 42, loss = 0.005305112915666928, eta = tensor([0.8066])\n",
      "ite = 43, loss = 0.005267604622298841, eta = tensor([0.8068])\n",
      "ite = 44, loss = 0.005231572792182812, eta = tensor([0.8070])\n",
      "ite = 45, loss = 0.005199058985674963, eta = tensor([0.8071])\n",
      "ite = 46, loss = 0.0051736282666617756, eta = tensor([0.8072])\n",
      "ite = 47, loss = 0.00514920036065153, eta = tensor([0.8071])\n",
      "ite = 48, loss = 0.005125873641290199, eta = tensor([0.8071])\n",
      "ite = 49, loss = 0.005103401044642474, eta = tensor([0.8070])\n",
      "ite = 50, loss = 0.005083265454908916, eta = tensor([0.8070])\n",
      "ite = 51, loss = 0.0050628903762714155, eta = tensor([0.8070])\n",
      "ite = 52, loss = 0.00503843926183499, eta = tensor([0.8071])\n",
      "ite = 53, loss = 0.005013101376274252, eta = tensor([0.8073])\n",
      "ite = 54, loss = 0.00498615446771446, eta = tensor([0.8075])\n",
      "ite = 55, loss = 0.004958746218129252, eta = tensor([0.8078])\n",
      "ite = 56, loss = 0.004930685091932627, eta = tensor([0.8081])\n",
      "ite = 57, loss = 0.004904095243727216, eta = tensor([0.8085])\n",
      "ite = 58, loss = 0.004879383317152345, eta = tensor([0.8088])\n",
      "ite = 59, loss = 0.004855976528923671, eta = tensor([0.8091])\n",
      "ite = 60, loss = 0.004834944997725293, eta = tensor([0.8094])\n",
      "ite = 61, loss = 0.004815943543563949, eta = tensor([0.8095])\n",
      "ite = 62, loss = 0.004797477933363459, eta = tensor([0.8097])\n",
      "ite = 63, loss = 0.004759628242002255, eta = tensor([0.8096])\n",
      "ite = 64, loss = 0.0046776906118596445, eta = tensor([0.8094])\n",
      "ite = 65, loss = 0.004576121720312801, eta = tensor([0.8091])\n",
      "ite = 66, loss = 0.004480031014974352, eta = tensor([0.8087])\n",
      "ite = 67, loss = 0.00437757022805574, eta = tensor([0.8084])\n",
      "ite = 68, loss = 0.0042862885900049955, eta = tensor([0.8080])\n",
      "ite = 69, loss = 0.004262111907053808, eta = tensor([0.8078])\n",
      "ite = 70, loss = 0.004249264610478197, eta = tensor([0.8077])\n",
      "ite = 71, loss = 0.004243599583069733, eta = tensor([0.8077])\n",
      "ite = 72, loss = 0.004234531380564233, eta = tensor([0.8079])\n",
      "ite = 73, loss = 0.0042224979837092004, eta = tensor([0.8080])\n",
      "ite = 74, loss = 0.004208040274768574, eta = tensor([0.8083])\n",
      "ite = 75, loss = 0.004192053510645331, eta = tensor([0.8086])\n",
      "ite = 76, loss = 0.004175793483227849, eta = tensor([0.8089])\n",
      "ite = 77, loss = 0.004159549166619235, eta = tensor([0.8093])\n",
      "ite = 78, loss = 0.004143956009551418, eta = tensor([0.8096])\n",
      "ite = 79, loss = 0.004129698706062881, eta = tensor([0.8100])\n",
      "ite = 80, loss = 0.00411544999803286, eta = tensor([0.8103])\n",
      "ite = 81, loss = 0.004101726613386735, eta = tensor([0.8107])\n",
      "ite = 82, loss = 0.004089229598282326, eta = tensor([0.8110])\n",
      "ite = 83, loss = 0.004076951641505197, eta = tensor([0.8113])\n",
      "ite = 84, loss = 0.004065324024438403, eta = tensor([0.8116])\n",
      "ite = 85, loss = 0.004054142025670246, eta = tensor([0.8119])\n",
      "ite = 86, loss = 0.004043337204059858, eta = tensor([0.8121])\n",
      "ite = 87, loss = 0.004032914482761288, eta = tensor([0.8123])\n",
      "ite = 88, loss = 0.004023041345278906, eta = tensor([0.8125])\n",
      "ite = 89, loss = 0.004013805463037941, eta = tensor([0.8127])\n",
      "ite = 90, loss = 0.0040050400860594335, eta = tensor([0.8128])\n",
      "ite = 91, loss = 0.003996190592100271, eta = tensor([0.8130])\n",
      "ite = 92, loss = 0.0039875669466054794, eta = tensor([0.8130])\n",
      "ite = 93, loss = 0.003978724257216938, eta = tensor([0.8132])\n",
      "ite = 94, loss = 0.0039710718691915265, eta = tensor([0.8134])\n",
      "ite = 95, loss = 0.003963211536453544, eta = tensor([0.8136])\n",
      "ite = 96, loss = 0.0039553123255007365, eta = tensor([0.8137])\n",
      "ite = 97, loss = 0.003946604606055195, eta = tensor([0.8138])\n",
      "ite = 98, loss = 0.003938318175108016, eta = tensor([0.8139])\n",
      "ite = 99, loss = 0.003930324556220196, eta = tensor([0.8140])\n",
      "ite = 100, loss = 0.003922692490063641, eta = tensor([0.8141])\n",
      "ite = 101, loss = 0.003915774122952197, eta = tensor([0.8141])\n",
      "ite = 102, loss = 0.003908649896191263, eta = tensor([0.8142])\n",
      "ite = 103, loss = 0.0039019851729874024, eta = tensor([0.8143])\n",
      "ite = 104, loss = 0.0038952877001788167, eta = tensor([0.8144])\n",
      "ite = 105, loss = 0.0038887565707224473, eta = tensor([0.8144])\n",
      "ite = 106, loss = 0.0038822654966122793, eta = tensor([0.8145])\n",
      "ite = 107, loss = 0.0038756559262860106, eta = tensor([0.8146])\n",
      "ite = 108, loss = 0.003869287867310893, eta = tensor([0.8147])\n",
      "ite = 109, loss = 0.003863204016028434, eta = tensor([0.8148])\n",
      "ite = 110, loss = 0.003857147931653121, eta = tensor([0.8148])\n",
      "ite = 111, loss = 0.003851333107236868, eta = tensor([0.8149])\n",
      "ite = 112, loss = 0.0038455509892320666, eta = tensor([0.8150])\n",
      "ite = 113, loss = 0.003840084582770372, eta = tensor([0.8151])\n",
      "ite = 114, loss = 0.003834726683384696, eta = tensor([0.8153])\n",
      "ite = 115, loss = 0.0038292881248732273, eta = tensor([0.8154])\n",
      "ite = 116, loss = 0.0038240905437568647, eta = tensor([0.8155])\n",
      "ite = 117, loss = 0.0038188309816515964, eta = tensor([0.8156])\n",
      "ite = 118, loss = 0.0038133495618883602, eta = tensor([0.8157])\n",
      "ite = 119, loss = 0.003808043409364084, eta = tensor([0.8159])\n",
      "ite = 120, loss = 0.0038027610706466196, eta = tensor([0.8160])\n",
      "ite = 121, loss = 0.0037976793581154466, eta = tensor([0.8161])\n",
      "ite = 122, loss = 0.0037926788562762034, eta = tensor([0.8162])\n",
      "ite = 123, loss = 0.003787580453731593, eta = tensor([0.8163])\n",
      "ite = 124, loss = 0.003782385901054162, eta = tensor([0.8164])\n",
      "ite = 125, loss = 0.003777581213814272, eta = tensor([0.8165])\n",
      "ite = 126, loss = 0.0037728754074913724, eta = tensor([0.8166])\n",
      "ite = 127, loss = 0.0037681003373773213, eta = tensor([0.8167])\n",
      "ite = 128, loss = 0.003763479347877235, eta = tensor([0.8167])\n",
      "ite = 129, loss = 0.003758816736693165, eta = tensor([0.8168])\n",
      "ite = 130, loss = 0.003754126209754089, eta = tensor([0.8169])\n",
      "ite = 131, loss = 0.0037494439279327626, eta = tensor([0.8170])\n",
      "ite = 132, loss = 0.0037447501693827555, eta = tensor([0.8171])\n",
      "ite = 133, loss = 0.003740266665054632, eta = tensor([0.8172])\n",
      "ite = 134, loss = 0.0037355906700443234, eta = tensor([0.8173])\n",
      "ite = 135, loss = 0.0037310358433276527, eta = tensor([0.8174])\n",
      "ite = 136, loss = 0.003726446697256125, eta = tensor([0.8174])\n",
      "ite = 137, loss = 0.0037220672046601776, eta = tensor([0.8175])\n",
      "ite = 138, loss = 0.003717282925223642, eta = tensor([0.8175])\n",
      "ite = 139, loss = 0.003713885459443475, eta = tensor([0.8175])\n",
      "ite = 140, loss = 0.0037103899225407813, eta = tensor([0.8175])\n",
      "ite = 141, loss = 0.0037068734874883547, eta = tensor([0.8175])\n",
      "ite = 142, loss = 0.003703117879377568, eta = tensor([0.8176])\n",
      "ite = 143, loss = 0.003699086649558863, eta = tensor([0.8177])\n",
      "ite = 144, loss = 0.0036951063663604455, eta = tensor([0.8178])\n",
      "ite = 145, loss = 0.0036914576454116013, eta = tensor([0.8179])\n",
      "ite = 146, loss = 0.0036879379753268233, eta = tensor([0.8180])\n",
      "ite = 147, loss = 0.0036841576338605534, eta = tensor([0.8181])\n",
      "ite = 148, loss = 0.0036802373455862595, eta = tensor([0.8181])\n",
      "ite = 149, loss = 0.003676499125929404, eta = tensor([0.8182])\n",
      "ite = 150, loss = 0.003672270877373014, eta = tensor([0.8183])\n",
      "ite = 151, loss = 0.0036682426825869006, eta = tensor([0.8184])\n",
      "ite = 152, loss = 0.003664414349767573, eta = tensor([0.8184])\n",
      "ite = 153, loss = 0.0036604391851596547, eta = tensor([0.8185])\n",
      "ite = 154, loss = 0.0036563226860823065, eta = tensor([0.8187])\n",
      "ite = 155, loss = 0.0036523003254543365, eta = tensor([0.8188])\n",
      "ite = 156, loss = 0.0036482016358476395, eta = tensor([0.8190])\n",
      "ite = 157, loss = 0.0036443344434441373, eta = tensor([0.8191])\n",
      "ite = 158, loss = 0.0036404768409959935, eta = tensor([0.8193])\n",
      "ite = 159, loss = 0.0036367943627931077, eta = tensor([0.8194])\n",
      "ite = 160, loss = 0.003632916356138464, eta = tensor([0.8196])\n",
      "ite = 161, loss = 0.0036290194592683627, eta = tensor([0.8197])\n",
      "ite = 162, loss = 0.00362472580027909, eta = tensor([0.8198])\n",
      "ite = 163, loss = 0.0036206249184166832, eta = tensor([0.8199])\n",
      "ite = 164, loss = 0.003616352010565009, eta = tensor([0.8200])\n",
      "ite = 165, loss = 0.0036122721362119373, eta = tensor([0.8202])\n",
      "ite = 166, loss = 0.0036082633463057766, eta = tensor([0.8203])\n",
      "ite = 167, loss = 0.0036043792831234804, eta = tensor([0.8204])\n",
      "ite = 168, loss = 0.003600539395820357, eta = tensor([0.8205])\n",
      "ite = 169, loss = 0.0035968467439852444, eta = tensor([0.8206])\n",
      "ite = 170, loss = 0.003593465571709404, eta = tensor([0.8207])\n",
      "ite = 171, loss = 0.0035900131770472453, eta = tensor([0.8208])\n",
      "ite = 172, loss = 0.0035864440658821915, eta = tensor([0.8210])\n",
      "ite = 173, loss = 0.003582848789286452, eta = tensor([0.8211])\n",
      "ite = 174, loss = 0.00357929046873032, eta = tensor([0.8212])\n",
      "ite = 175, loss = 0.0035757297920125646, eta = tensor([0.8214])\n",
      "ite = 176, loss = 0.003572069646076515, eta = tensor([0.8215])\n",
      "ite = 177, loss = 0.0035687140473222353, eta = tensor([0.8216])\n",
      "ite = 178, loss = 0.003565015449791058, eta = tensor([0.8218])\n",
      "ite = 179, loss = 0.0035614985241570487, eta = tensor([0.8219])\n",
      "ite = 180, loss = 0.0035581782256364726, eta = tensor([0.8220])\n",
      "ite = 181, loss = 0.0035549919682741514, eta = tensor([0.8221])\n",
      "ite = 182, loss = 0.003551672011102452, eta = tensor([0.8223])\n",
      "ite = 183, loss = 0.003548377619803669, eta = tensor([0.8224])\n",
      "ite = 184, loss = 0.0035434617840902445, eta = tensor([0.8225])\n",
      "ite = 185, loss = 0.003536566598148572, eta = tensor([0.8226])\n",
      "ite = 186, loss = 0.003527724788326957, eta = tensor([0.8227])\n",
      "ite = 187, loss = 0.0035176621049325366, eta = tensor([0.8229])\n",
      "ite = 188, loss = 0.0035068924291752875, eta = tensor([0.8231])\n",
      "ite = 189, loss = 0.0034901392265902294, eta = tensor([0.8235])\n",
      "ite = 190, loss = 0.003469442742166286, eta = tensor([0.8239])\n",
      "ite = 191, loss = 0.0034236171480378115, eta = tensor([0.8248])\n",
      "ite = 192, loss = 0.0033379239949108304, eta = tensor([0.8259])\n",
      "ite = 193, loss = 0.003257679506807133, eta = tensor([0.8270])\n",
      "ite = 194, loss = 0.00320440426401119, eta = tensor([0.8278])\n",
      "ite = 195, loss = 0.0031653345327403793, eta = tensor([0.8282])\n",
      "ite = 196, loss = 0.0031086329456410915, eta = tensor([0.8282])\n",
      "ite = 197, loss = 0.0030337109314511907, eta = tensor([0.8279])\n",
      "ite = 198, loss = 0.0029714296976911755, eta = tensor([0.8277])\n",
      "ite = 199, loss = 0.0029389721124098342, eta = tensor([0.8277])\n",
      "ite = 200, loss = 0.0029192736312942252, eta = tensor([0.8280])\n",
      "ite = 201, loss = 0.002890674833706953, eta = tensor([0.8285])\n",
      "ite = 202, loss = 0.002849418778386267, eta = tensor([0.8292])\n",
      "ite = 203, loss = 0.0028119662782511027, eta = tensor([0.8300])\n",
      "ite = 204, loss = 0.0027898821820839764, eta = tensor([0.8306])\n",
      "ite = 205, loss = 0.00278876008642393, eta = tensor([0.8310])\n",
      "ite = 206, loss = 0.002784569339102312, eta = tensor([0.8310])\n",
      "ite = 207, loss = 0.0027715464346755006, eta = tensor([0.8308])\n",
      "ite = 208, loss = 0.0027544373587466446, eta = tensor([0.8300])\n",
      "ite = 209, loss = 0.00274506351723595, eta = tensor([0.8293])\n",
      "ite = 210, loss = 0.002751338295559915, eta = tensor([0.8289])\n",
      "ite = 211, loss = 0.0027527807378205918, eta = tensor([0.8287])\n",
      "ite = 212, loss = 0.0027478778731536155, eta = tensor([0.8288])\n",
      "ite = 213, loss = 0.0027367841255508075, eta = tensor([0.8290])\n",
      "ite = 214, loss = 0.0027220014095699356, eta = tensor([0.8293])\n",
      "ite = 215, loss = 0.0027053593890152836, eta = tensor([0.8298])\n",
      "ite = 216, loss = 0.0026891096717842978, eta = tensor([0.8303])\n",
      "ite = 217, loss = 0.002675054475113211, eta = tensor([0.8308])\n",
      "ite = 218, loss = 0.002667084888202699, eta = tensor([0.8311])\n",
      "ite = 219, loss = 0.002662475170596252, eta = tensor([0.8314])\n",
      "ite = 220, loss = 0.0026570135922302156, eta = tensor([0.8315])\n",
      "ite = 221, loss = 0.002648390301227617, eta = tensor([0.8314])\n",
      "ite = 222, loss = 0.002637140418857978, eta = tensor([0.8312])\n",
      "ite = 223, loss = 0.0026267771525656253, eta = tensor([0.8310])\n",
      "ite = 224, loss = 0.0026194828678117385, eta = tensor([0.8309])\n",
      "ite = 225, loss = 0.002614155494399344, eta = tensor([0.8308])\n",
      "ite = 226, loss = 0.0026093046778803254, eta = tensor([0.8307])\n",
      "ite = 227, loss = 0.00260463720335287, eta = tensor([0.8308])\n",
      "ite = 228, loss = 0.0025997055602951435, eta = tensor([0.8309])\n",
      "ite = 229, loss = 0.0025943511585629336, eta = tensor([0.8311])\n",
      "ite = 230, loss = 0.0025887626398753884, eta = tensor([0.8313])\n",
      "ite = 231, loss = 0.002582858543368623, eta = tensor([0.8316])\n",
      "ite = 232, loss = 0.002577200615379344, eta = tensor([0.8319])\n",
      "ite = 233, loss = 0.0025719706039321766, eta = tensor([0.8321])\n",
      "ite = 234, loss = 0.0025671232214126757, eta = tensor([0.8324])\n",
      "ite = 235, loss = 0.0025629294208716078, eta = tensor([0.8326])\n",
      "ite = 236, loss = 0.0025591629620983656, eta = tensor([0.8327])\n",
      "ite = 237, loss = 0.0025554712948934154, eta = tensor([0.8328])\n",
      "ite = 238, loss = 0.002551142017126814, eta = tensor([0.8327])\n",
      "ite = 239, loss = 0.002546896736504666, eta = tensor([0.8327])\n",
      "ite = 240, loss = 0.0025429302763640936, eta = tensor([0.8326])\n",
      "ite = 241, loss = 0.002539590215096728, eta = tensor([0.8326])\n",
      "ite = 242, loss = 0.0025367920953181552, eta = tensor([0.8326])\n",
      "ite = 243, loss = 0.002534038625983325, eta = tensor([0.8327])\n",
      "ite = 244, loss = 0.002531164488071061, eta = tensor([0.8327])\n",
      "ite = 245, loss = 0.002528431034160323, eta = tensor([0.8328])\n",
      "ite = 246, loss = 0.0025254275833631475, eta = tensor([0.8330])\n",
      "ite = 247, loss = 0.0025221646174294066, eta = tensor([0.8332])\n",
      "ite = 248, loss = 0.002519507522922342, eta = tensor([0.8333])\n",
      "ite = 249, loss = 0.0025166881085392544, eta = tensor([0.8335])\n",
      "ite = 250, loss = 0.002514109956270819, eta = tensor([0.8336])\n",
      "ite = 251, loss = 0.002511407947002402, eta = tensor([0.8337])\n",
      "ite = 252, loss = 0.002508653437605438, eta = tensor([0.8337])\n",
      "ite = 253, loss = 0.002505806293630267, eta = tensor([0.8338])\n",
      "ite = 254, loss = 0.0025030719536512465, eta = tensor([0.8338])\n",
      "ite = 255, loss = 0.002500671997361245, eta = tensor([0.8339])\n",
      "ite = 256, loss = 0.002497969897695145, eta = tensor([0.8340])\n",
      "ite = 257, loss = 0.0024953023821591134, eta = tensor([0.8341])\n",
      "ite = 258, loss = 0.002492500296581701, eta = tensor([0.8342])\n",
      "ite = 259, loss = 0.0024898050172786016, eta = tensor([0.8344])\n",
      "ite = 260, loss = 0.002487014872896108, eta = tensor([0.8345])\n",
      "ite = 261, loss = 0.0024844664349063976, eta = tensor([0.8346])\n",
      "ite = 262, loss = 0.002482157885438911, eta = tensor([0.8347])\n",
      "ite = 263, loss = 0.002480005276204891, eta = tensor([0.8348])\n",
      "ite = 264, loss = 0.0024775802224327953, eta = tensor([0.8348])\n",
      "ite = 265, loss = 0.0024752643844580995, eta = tensor([0.8349])\n",
      "ite = 266, loss = 0.00247310830606684, eta = tensor([0.8350])\n",
      "ite = 267, loss = 0.002471013604597716, eta = tensor([0.8350])\n",
      "ite = 268, loss = 0.0024688721798234096, eta = tensor([0.8351])\n",
      "ite = 269, loss = 0.0024668952484858545, eta = tensor([0.8352])\n",
      "ite = 270, loss = 0.002464732684279625, eta = tensor([0.8353])\n",
      "ite = 271, loss = 0.0024628802778197435, eta = tensor([0.8354])\n",
      "ite = 272, loss = 0.0024608774361719126, eta = tensor([0.8354])\n",
      "ite = 273, loss = 0.002458830076389328, eta = tensor([0.8355])\n",
      "ite = 274, loss = 0.002456962435118273, eta = tensor([0.8355])\n",
      "ite = 275, loss = 0.0024551015572359704, eta = tensor([0.8356])\n",
      "ite = 276, loss = 0.002453203453902828, eta = tensor([0.8357])\n",
      "ite = 277, loss = 0.002451474855222233, eta = tensor([0.8358])\n",
      "ite = 278, loss = 0.0024497174370309016, eta = tensor([0.8358])\n",
      "ite = 279, loss = 0.002447871468276841, eta = tensor([0.8359])\n",
      "ite = 280, loss = 0.002446205412948005, eta = tensor([0.8359])\n",
      "ite = 281, loss = 0.0024445576150021807, eta = tensor([0.8360])\n",
      "ite = 282, loss = 0.002442704870484793, eta = tensor([0.8361])\n",
      "ite = 283, loss = 0.0024410261965537175, eta = tensor([0.8361])\n",
      "ite = 284, loss = 0.002439265367092847, eta = tensor([0.8362])\n",
      "ite = 285, loss = 0.0024376270226358016, eta = tensor([0.8363])\n",
      "ite = 286, loss = 0.0024359874637174313, eta = tensor([0.8363])\n",
      "ite = 287, loss = 0.0024343965465949504, eta = tensor([0.8364])\n",
      "ite = 288, loss = 0.002432764656415682, eta = tensor([0.8364])\n",
      "ite = 289, loss = 0.002431130374104788, eta = tensor([0.8365])\n",
      "ite = 290, loss = 0.0024296105891602688, eta = tensor([0.8366])\n",
      "ite = 291, loss = 0.002427874397052875, eta = tensor([0.8367])\n",
      "ite = 292, loss = 0.0024265107176154087, eta = tensor([0.8367])\n",
      "ite = 293, loss = 0.0024249829389657754, eta = tensor([0.8368])\n",
      "ite = 294, loss = 0.0024234322766916085, eta = tensor([0.8369])\n",
      "ite = 295, loss = 0.0024219226656758005, eta = tensor([0.8369])\n",
      "ite = 296, loss = 0.0024203207901801367, eta = tensor([0.8370])\n",
      "ite = 297, loss = 0.0024188294382489245, eta = tensor([0.8371])\n",
      "ite = 298, loss = 0.0024173547617982144, eta = tensor([0.8372])\n",
      "ite = 299, loss = 0.0024157881312294473, eta = tensor([0.8372])\n",
      "ite = 300, loss = 0.0024144303909558592, eta = tensor([0.8373])\n",
      "ite = 301, loss = 0.0024129272753785277, eta = tensor([0.8374])\n",
      "ite = 302, loss = 0.0024114163864866517, eta = tensor([0.8374])\n",
      "ite = 303, loss = 0.0024101153797773, eta = tensor([0.8375])\n",
      "ite = 304, loss = 0.00240862097720727, eta = tensor([0.8375])\n",
      "ite = 305, loss = 0.0024072227120690497, eta = tensor([0.8376])\n",
      "ite = 306, loss = 0.002405955022291882, eta = tensor([0.8377])\n",
      "ite = 307, loss = 0.0024045739221450722, eta = tensor([0.8377])\n",
      "ite = 308, loss = 0.0024031419835726636, eta = tensor([0.8377])\n",
      "ite = 309, loss = 0.0024018483407548695, eta = tensor([0.8378])\n",
      "ite = 310, loss = 0.002400654494048554, eta = tensor([0.8379])\n",
      "ite = 311, loss = 0.0023990925902232845, eta = tensor([0.8380])\n",
      "ite = 312, loss = 0.002397975334676256, eta = tensor([0.8380])\n",
      "ite = 313, loss = 0.0023966697897277157, eta = tensor([0.8381])\n",
      "ite = 314, loss = 0.00239524461510496, eta = tensor([0.8381])\n",
      "ite = 315, loss = 0.002394021395976999, eta = tensor([0.8382])\n",
      "ite = 316, loss = 0.0023928097521162843, eta = tensor([0.8383])\n",
      "ite = 317, loss = 0.002391485147770543, eta = tensor([0.8383])\n",
      "ite = 318, loss = 0.0023902001868614485, eta = tensor([0.8384])\n",
      "ite = 319, loss = 0.00238896768544828, eta = tensor([0.8384])\n",
      "ite = 320, loss = 0.0023876691752642066, eta = tensor([0.8385])\n",
      "ite = 321, loss = 0.002386417039790865, eta = tensor([0.8386])\n",
      "ite = 322, loss = 0.0023853174197749727, eta = tensor([0.8386])\n",
      "ite = 323, loss = 0.002384106770871701, eta = tensor([0.8387])\n",
      "ite = 324, loss = 0.002382857269385099, eta = tensor([0.8388])\n",
      "ite = 325, loss = 0.0023815596732693523, eta = tensor([0.8388])\n",
      "ite = 326, loss = 0.0023804853058434013, eta = tensor([0.8389])\n",
      "ite = 327, loss = 0.002379311435214195, eta = tensor([0.8389])\n",
      "ite = 328, loss = 0.00237841554876811, eta = tensor([0.8390])\n",
      "ite = 329, loss = 0.002377239541660531, eta = tensor([0.8391])\n",
      "ite = 330, loss = 0.0023761531289321824, eta = tensor([0.8392])\n",
      "ite = 331, loss = 0.002375223848546509, eta = tensor([0.8392])\n",
      "ite = 332, loss = 0.002373987250831347, eta = tensor([0.8393])\n",
      "ite = 333, loss = 0.002372879883821426, eta = tensor([0.8393])\n",
      "ite = 334, loss = 0.0023718211359567854, eta = tensor([0.8394])\n",
      "ite = 335, loss = 0.002370790375616237, eta = tensor([0.8394])\n",
      "ite = 336, loss = 0.0023695743869530947, eta = tensor([0.8395])\n",
      "ite = 337, loss = 0.0023684066113450336, eta = tensor([0.8396])\n",
      "ite = 338, loss = 0.0023675187685755837, eta = tensor([0.8397])\n",
      "ite = 339, loss = 0.0023664731452462287, eta = tensor([0.8397])\n",
      "ite = 340, loss = 0.002365534604324923, eta = tensor([0.8397])\n",
      "ite = 341, loss = 0.0023648415970173317, eta = tensor([0.8398])\n",
      "ite = 342, loss = 0.0023639758917545738, eta = tensor([0.8398])\n",
      "ite = 343, loss = 0.002363162884782013, eta = tensor([0.8398])\n",
      "ite = 344, loss = 0.002362153144472267, eta = tensor([0.8399])\n",
      "ite = 345, loss = 0.002361039848048363, eta = tensor([0.8400])\n",
      "ite = 346, loss = 0.0023599011547028714, eta = tensor([0.8401])\n",
      "ite = 347, loss = 0.0023586204631170435, eta = tensor([0.8402])\n",
      "ite = 348, loss = 0.002358069415932914, eta = tensor([0.8402])\n",
      "ite = 349, loss = 0.002356848446241658, eta = tensor([0.8402])\n",
      "ite = 350, loss = 0.00235547641772523, eta = tensor([0.8403])\n",
      "ite = 351, loss = 0.002354559452252357, eta = tensor([0.8403])\n",
      "ite = 352, loss = 0.0023535342451743773, eta = tensor([0.8404])\n",
      "ite = 353, loss = 0.0023523091277889542, eta = tensor([0.8405])\n",
      "ite = 354, loss = 0.0023513180247211624, eta = tensor([0.8406])\n",
      "ite = 355, loss = 0.0023506690512464677, eta = tensor([0.8406])\n",
      "ite = 356, loss = 0.002349414105426446, eta = tensor([0.8406])\n",
      "ite = 357, loss = 0.0023486007297831117, eta = tensor([0.8407])\n",
      "ite = 358, loss = 0.0023477325787460744, eta = tensor([0.8407])\n",
      "ite = 359, loss = 0.0023469323547635323, eta = tensor([0.8408])\n",
      "ite = 360, loss = 0.0023460747298871235, eta = tensor([0.8409])\n",
      "ite = 361, loss = 0.002345147285545201, eta = tensor([0.8410])\n",
      "ite = 362, loss = 0.002344132442188741, eta = tensor([0.8411])\n",
      "ite = 363, loss = 0.002343202525195736, eta = tensor([0.8412])\n",
      "ite = 364, loss = 0.0023423224021805857, eta = tensor([0.8412])\n",
      "ite = 365, loss = 0.0023413903405372794, eta = tensor([0.8412])\n",
      "ite = 366, loss = 0.002340536633927748, eta = tensor([0.8413])\n",
      "ite = 367, loss = 0.002339744272494549, eta = tensor([0.8413])\n",
      "ite = 368, loss = 0.0023388378853514704, eta = tensor([0.8414])\n",
      "ite = 369, loss = 0.00233786621865586, eta = tensor([0.8415])\n",
      "ite = 370, loss = 0.002337019564591679, eta = tensor([0.8416])\n",
      "ite = 371, loss = 0.002336415281778497, eta = tensor([0.8416])\n",
      "ite = 372, loss = 0.0023353175154284104, eta = tensor([0.8416])\n",
      "ite = 373, loss = 0.0023345601176869074, eta = tensor([0.8417])\n",
      "ite = 374, loss = 0.0023337986770379836, eta = tensor([0.8417])\n",
      "ite = 375, loss = 0.0023329996077563493, eta = tensor([0.8418])\n",
      "ite = 376, loss = 0.0023320946044074254, eta = tensor([0.8419])\n",
      "ite = 377, loss = 0.002331266036794289, eta = tensor([0.8420])\n",
      "ite = 378, loss = 0.0023303696575586418, eta = tensor([0.8421])\n",
      "ite = 379, loss = 0.0023298164635097117, eta = tensor([0.8421])\n",
      "ite = 380, loss = 0.0023287835348608163, eta = tensor([0.8421])\n",
      "ite = 381, loss = 0.0023279916782245206, eta = tensor([0.8422])\n",
      "ite = 382, loss = 0.0023273354630726903, eta = tensor([0.8422])\n",
      "ite = 383, loss = 0.0023266061593455066, eta = tensor([0.8423])\n",
      "ite = 384, loss = 0.00232605761134529, eta = tensor([0.8423])\n"
     ]
    }
   ],
   "source": [
    "df_dp = np.load(\"dataset/data_N365_0.npz\")\n",
    "df_price = df_dp[\"price\"]\n",
    "\n",
    "T = 24\n",
    "N_train = 20\n",
    "P1 = df_dp[\"p\"].max()\n",
    "P2 = df_dp[\"d\"].max()\n",
    "d = df_dp[\"d\"]\n",
    "p = df_dp[\"p\"]\n",
    "price_tensor = torch.from_numpy(df_price[10:10+N_train]).double()\n",
    "d_tensor = torch.from_numpy(d[10:10+N_train]).double()\n",
    "p_tensor = torch.from_numpy(p[10:10+N_train]).double()\n",
    "y_tensor = tuple([d_tensor, p_tensor])\n",
    "\n",
    "price_tensor2 = torch.from_numpy(df_price[:10]).double()\n",
    "d_tensor2 = torch.from_numpy(d[:10]).double()\n",
    "p_tensor2 = torch.from_numpy(p[:10]).double()\n",
    "y_tensor2 = tuple([d_tensor2, p_tensor2])\n",
    "%matplotlib auto\n",
    "L = []\n",
    "val_L = []\n",
    "torch.manual_seed(0)\n",
    "layer = DRagent(P1, P2, T)\n",
    "opt1 = optim.Adam(layer.parameters(), lr=1e-2)\n",
    "for ite in range(500):\n",
    "    dp_pred = layer(price_tensor, d_tensor, ite)\n",
    "    if ite == 10:\n",
    "        opt1.param_groups[0][\"lr\"] = 5e-3\n",
    "    loss = nn.MSELoss()(y_tensor[0], dp_pred[0]) + nn.MSELoss()(y_tensor[1], dp_pred[1])\n",
    "    opt1.zero_grad()\n",
    "    loss.backward()\n",
    "    opt1.step()\n",
    "    with torch.no_grad():\n",
    "        layer.E1.data = torch.clamp(layer.E1.data, min=0.01, max=100) \n",
    "        layer.E2.data = torch.clamp(layer.E2.data, min=-100, max=-0.01) \n",
    "        layer.eta.data =  torch.clamp(layer.eta.data, min=0.5, max=1) \n",
    "    layer.eval()\n",
    "    dp_pred2 = layer(price_tensor2, d_tensor2, 0)\n",
    "    loss2 = nn.MSELoss()(y_tensor2[0], dp_pred2[0]) + nn.MSELoss()(y_tensor2[1], dp_pred2[1])\n",
    "    val_L.append(loss2.detach().numpy())\n",
    "    print(f'ite = {ite}, loss = {loss.detach().numpy()}, eta = {layer.eta.data}')\n",
    "    L.append(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "import plotly\n",
    "from plotly.express.colors import sample_colorscale\n",
    "x = np.linspace(0, 1, 25)\n",
    "c = sample_colorscale('Greys', list(x))\n",
    "fig = go.Figure()\n",
    "for i in range(2,15):\n",
    "    fig.add_trace(go.Scatter(y=[layer.record[j]/abs(layer.record[i*10]) for j in range(i*10,(i+1)*10)], showlegend=False, line={'color':c[i+5]}))\n",
    "fig.update_layout({\n",
    "    'xaxis':{'title': r\"k\", 'showline':True},\n",
    "    'yaxis':{'title': r'$c(y^{(k)})/|c(y^{(0)})|$'},\n",
    "    #'yaxis2': {'title':'Price ($/MWh)','anchor':'x', 'overlaying':'y', 'side':'right', 'range':[0,40],'showgrid':False},\n",
    "    \"font\": {\"size\": 20,'color':'#3a4142'},\n",
    "    'template':'plotly_white',\n",
    "    'legend':{'yanchor':'bottom','y':1,'xanchor':'left','x':0.0,'orientation':'h'},\n",
    "   'width':800,\n",
    "    'height':500,\n",
    "    'coloraxis': {'colorscale':'viridis'}\n",
    "})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"result/images/convergence.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_d, pred_p = layer(price_tensor, d_tensor, 500)\n",
    "pred_d = pred_d.detach().numpy()\n",
    "pred_p = pred_p.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df_dp[\"d\"]\n",
    "p = df_dp[\"p\"]\n",
    "d2 = df_dp[\"d\"][:10]\n",
    "p2 = df_dp[\"p\"][:10]\n",
    "price_tensor2 = torch.from_numpy(df_price[:10]).double()\n",
    "d_tensor2 = torch.from_numpy(d[:10]).double()\n",
    "p_tensor2 = torch.from_numpy(p[:10]).double()\n",
    "pred_d2, pred_p2 = layer(price_tensor2, d_tensor2, 500)\n",
    "pred_d2 = pred_d2.detach().numpy()\n",
    "pred_p2 = pred_p2.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2, c1 = layer.approximate_cost(d_tensor, diff=True)\n",
    "torch.mean(torch.diag(c2[5])), torch.mean(c1[5]), layer.E1.data, layer.E2.data, layer.eta.data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(layer.state_dict(), '../EnergyStorage/result/model_gradient/ICNN_vector/model_gradient_model_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/bb/Desktop/UCSD/research/E2E/e2e-DR-learning/energystorage-model/Results/OptNet_val_loss.csv')\n",
    "opt_quad = df.values[:,1:]\n",
    "opt_quad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m val_loss_mlp \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m500\u001b[39m))\n\u001b[1;32m      2\u001b[0m val_loss_rnn \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m500\u001b[39m))\n\u001b[1;32m      3\u001b[0m val_loss_model \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m500\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "val_loss_mlp = np.zeros((10, 500))\n",
    "val_loss_rnn = np.zeros((10, 500))\n",
    "val_loss_model = np.zeros((10, 500))\n",
    "for i in range(10):\n",
    "    val_loss_mlp[i] = np.load(f'result/baseline/MLP_loss_{i}.npz')['val_loss']\n",
    "    val_loss_rnn[i] = np.load(f'result/baseline/RNN_loss_{i}.npz')['val_loss']\n",
    "for i in range(10):\n",
    "    val_loss_model[i] = np.load(f'result/model_gradient/loss_{i}.npz')['val_loss']\n",
    "\n",
    "fig = go.Figure()\n",
    "x1 = np.linspace(0,500,100)\n",
    "x2 = np.linspace(0,500,100)\n",
    "x3 = np.linspace(0,500,6)\n",
    "\n",
    "median_NN =  np.median(val_loss_mlp, axis=0)[:500]\n",
    "median_NN_2 = np.quantile(val_loss_mlp,0.2, axis=0)[:500]\n",
    "median_NN_8 = np.quantile(val_loss_mlp,0.8,axis=0)[:500]\n",
    "\n",
    "median_RNN =  np.median(val_loss_rnn, axis=0)[:500]\n",
    "median_RNN_2 = np.quantile(val_loss_rnn,0.2,axis=0)[:500]\n",
    "median_RNN_8 = np.quantile(val_loss_rnn,0.8,axis=0)[:500]\n",
    "\n",
    "median_opt0 =  val_L#np.median(opt_quad, axis=0)\n",
    "median_opt0_2 = np.quantile(opt_quad, 0.2, axis=0)\n",
    "median_opt0_8 = np.quantile(opt_quad, 0.8, axis=0)\n",
    "\n",
    "median_opt1 =  np.median(val_loss_model, axis=0)[:500]\n",
    "median_opt1_2 = np.quantile(val_loss_model, 0.2, axis=0)\n",
    "median_opt1_8 = np.quantile(val_loss_model, 0.8, axis=0)\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x1, y=median_NN_2,fill=None,mode='lines',line_color='rgba(0,0,0,0)', showlegend = False ))\n",
    "fig.add_trace(go.Scatter(x=x1,y=median_NN_8,fill='tonexty', mode='lines',  fillcolor='rgba(203,158,252,0.2)',line_color = 'rgba(0,0,0,0)',showlegend = False))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x1, y=median_RNN_2,fill=None,mode='lines',line_color='rgba(0,0,0,0)', showlegend = False ))\n",
    "fig.add_trace(go.Scatter(x=x1,y=median_RNN_8,fill='tonexty', mode='lines',  fillcolor='rgba(168,236,217,0.5)',line_color='rgba(0,0,0,0)',showlegend = False))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x2, y=median_opt1_2,fill=None,mode='lines',line_color='rgba(0,0,0,0)', showlegend = False    ))\n",
    "fig.add_trace(go.Scatter(x=x2,y=median_opt1_8,fill='tonexty', mode='lines',  fillcolor='rgba(240,96,72,0.5)',line_color = 'rgba(0,0,0,0)',showlegend = False))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x3, y=median_opt0_2,fill=None,mode='lines',line_color='rgba(0,0,0,0)', showlegend = False    ))\n",
    "fig.add_trace(go.Scatter(x=x3,y=median_opt0_8,fill='tonexty', mode='lines',  fillcolor='rgba(250,232,20,0.5)',line_color = 'rgba(0,0,0,0)',showlegend = False))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x1,y=median_NN, mode='lines',  line_color='#b681f0',opacity=0.5, line_width=3,name = 'median(MLP,300)')) \n",
    "fig.add_trace(go.Scatter(x=x1,y=median_RNN, mode='lines',  line_color='#07cd98',opacity=0.7,line_width=3,name = 'median(RNN,300)')) \n",
    "fig.add_trace(go.Scatter(x=x2,y=median_opt1, mode='lines',  line_color='#f06048',opacity=0.5,line_width=3,name = 'median(Ours-nonconvex,20)')) \n",
    "fig.add_trace(go.Scatter(x=x1,y=median_opt0, mode='lines',  line_color='#fff50e',opacity=0.5,line_width=3,name = 'median(Ours-convex,20)')) \n",
    "\n",
    "fig.update_layout({#'title': 'NRMSE comparison',\n",
    "    'xaxis':{'title':'Iterations', 'showline':True},\n",
    "    'yaxis':{'title': 'Validation Loss',\"range\":[0,0.05]},\n",
    "    \"font\": {\"size\": 40, 'color':'#3a4142'},\n",
    "    'template':'plotly_white',\n",
    "    'legend':{'yanchor':'bottom','y':1,'xanchor':'left','x':0.0,'orientation':'h'},\n",
    "    'width':1600,\n",
    "   'height':900\n",
    "})\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"result/images/validation_loss.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"result/images/test_1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnmodel = keras.models.load_model('result/baseline/model_RNN')\n",
    "mlpmodel = keras.models.load_model('result/baseline/model_MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(10)\n",
    "d = df_dp[\"d\"]\n",
    "p = df_dp[\"p\"]\n",
    "test_price = df_price[:10]\n",
    "\n",
    "dr1 = rnnmodel(df_price[i:i+1])\n",
    "dr3 = mlpmodel(df_price[i:i+1])\n",
    "test_dr = p[:10] - d[:10]\n",
    "pred_dr = pred_p2 - pred_d2 \n",
    "t1 = go.Scatter(x=np.arange(24), y=test_price[i],mode='lines', name='price',line = {'width':6,'dash':'dash','color':'#ffa05a'},yaxis='y2')\n",
    "t2 = go.Scatter(x=np.arange(24), y=test_dr[i],mode='lines', name='True',line = {'width':6,'dash':'dash'})\n",
    "t5 = go.Scatter(x=np.arange(24), y=pred_dr[i],mode='lines', name='Ours(non-convex)',line = {'width':4})\n",
    "t3 = go.Scatter(x=np.arange(24), y=dr1[0],mode='lines', name='RNN(300)',line = {'width':4})\n",
    "t6 = go.Scatter(x=np.arange(24), y=dr3[0],mode='lines', name='MLP(300)',line = {'width':4})\n",
    "fig = go.Figure(data=[t2,t5, t3,t6,t1],layout={\n",
    "    'xaxis':{'title':'Hour', 'showline':True},\n",
    "    'yaxis':{'title': 'Dispatch(MW)','range':[-0.6,0.5]},\n",
    "    'yaxis2': {'title':'Price ($/MWh)','anchor':'x', 'overlaying':'y', 'side':'right','range':[10,100], 'showgrid':False},\n",
    "    \"font\": {\"size\": 40,'color':'#3a4142'},\n",
    "    'template':'plotly_white',\n",
    "    'legend':{'yanchor':'bottom','y':1,'xanchor':'left','x':0.0,'orientation':'h'},\n",
    "    'width':1600,\n",
    "    'height':900\n",
    "})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"result/images/predict.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization with matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 24\n",
    "np.random.seed(1)\n",
    "E1 = 0.5\n",
    "E2 = -0.5\n",
    "eta = 0.9\n",
    "eps = 1e-5\n",
    "\n",
    "obj = (\n",
    "    lambda d, p, price, c1, c2, E1, E2, eta: -price @ (d - p)\n",
    "    + c1 @ d\n",
    "    + cp.quad_form(d, c2)\n",
    "    if isinstance(d, cp.Variable) #cp.sum_squares(cp.sqrt(cp.diag(c2)) @ d)\n",
    "    else -price @ (d - p) + c1 @ d +  d @ c2 @ d #torch.sum((torch.sqrt(torch.diag(c2)) @ d)**2)\n",
    ")\n",
    "\n",
    "ineq1 = (lambda d, p, price, c1, c2, E1, E2, eta: p - torch.ones(T, dtype=torch.double) * P1)\n",
    "ineq2 = (lambda d, p, price, c1, c2, E1, E2, eta: torch.ones(T, dtype=torch.double)* 0 - p)\n",
    "ineq3 = (lambda d, p, price, c1, c2, E1, E2, eta: d - torch.ones(T, dtype=torch.double) * P2)\n",
    "ineq4 = (lambda d, p, price, c1, c2, E1, E2, eta: torch.ones(T, dtype=torch.double) * 0 - d)\n",
    "ineq5 = (lambda d, p, price, c1, c2, E1, E2, eta: torch.tril(torch.ones(T, T, dtype=torch.double)) @ (eta * p - d / eta)\n",
    "    - torch.as_tensor(np.arange(eps, (T+1)*eps, eps))\n",
    "    - torch.ones(T, dtype=torch.double) * E1)\n",
    "ineq6 = lambda d, p, price, c1, c2, E1, E2, eta: torch.ones(\n",
    "    T, dtype=torch.double\n",
    ") * E2 - torch.tril(torch.ones(T, T, dtype=torch.double)) @ (eta * p - d / eta) + torch.as_tensor(np.arange(eps, (T+1)*eps, eps))\n",
    "\n",
    "# Define and solve the CVXPY problem.\n",
    "p, d = cp.Variable(T),cp.Variable(T)\n",
    "price, c1, c2 = cp.Parameter(T), cp.Parameter(T), cp.Parameter((T,T), symmetric=True)\n",
    "price = price_tensor[0].detach().numpy()\n",
    "c1 = grad[0].detach().numpy()\n",
    "c2 = hessian[0].detach().numpy()\n",
    "prob = cp.Problem(cp.Minimize(-price @ (d - p)\n",
    "    + c1 @ d\n",
    "    + cp.quad_form(d, c2)),[ineq1,ineq2,ineq3,ineq4,ineq5,ineq6])\n",
    "prob.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5cc9548c05a244386a6d3194fd4ce0daff846e301aca00a00f7a9d28040987a3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('dr': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
